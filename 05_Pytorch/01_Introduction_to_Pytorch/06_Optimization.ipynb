{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Model Parameters\n",
    "\n",
    "Training a model is an iterative process, in each iteration the model makes a guess about the output, calculates the error in its guess (loss), collects the derivatives of the error with espect to its parameters, and optimizes these parameters using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"~/Developer/Datasets\", \n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"~/Developer/Datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "print(next(iter(train_dataloader))[0].shape)\n",
    "print(next(iter(train_dataloader))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=28*28, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # Initialize the loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Initialize the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.274175 [    0/60000]\n",
      "loss: 1.405884 [ 6400/60000]\n",
      "loss: 0.939259 [12800/60000]\n",
      "loss: 1.090776 [19200/60000]\n",
      "loss: 0.869180 [25600/60000]\n",
      "loss: 0.843452 [32000/60000]\n",
      "loss: 0.772076 [38400/60000]\n",
      "loss: 0.720589 [44800/60000]\n",
      "loss: 0.744764 [51200/60000]\n",
      "loss: 0.742828 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.665286 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.627443 [    0/60000]\n",
      "loss: 0.699240 [ 6400/60000]\n",
      "loss: 0.448176 [12800/60000]\n",
      "loss: 0.742594 [19200/60000]\n",
      "loss: 0.650697 [25600/60000]\n",
      "loss: 0.614572 [32000/60000]\n",
      "loss: 0.588853 [38400/60000]\n",
      "loss: 0.612970 [44800/60000]\n",
      "loss: 0.638653 [51200/60000]\n",
      "loss: 0.602008 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.551951 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.506269 [    0/60000]\n",
      "loss: 0.578354 [ 6400/60000]\n",
      "loss: 0.364903 [12800/60000]\n",
      "loss: 0.642414 [19200/60000]\n",
      "loss: 0.569135 [25600/60000]\n",
      "loss: 0.530781 [32000/60000]\n",
      "loss: 0.504354 [38400/60000]\n",
      "loss: 0.576638 [44800/60000]\n",
      "loss: 0.590042 [51200/60000]\n",
      "loss: 0.527910 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.500634 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.448536 [    0/60000]\n",
      "loss: 0.514791 [ 6400/60000]\n",
      "loss: 0.321966 [12800/60000]\n",
      "loss: 0.584289 [19200/60000]\n",
      "loss: 0.518291 [25600/60000]\n",
      "loss: 0.484060 [32000/60000]\n",
      "loss: 0.449291 [38400/60000]\n",
      "loss: 0.553164 [44800/60000]\n",
      "loss: 0.553695 [51200/60000]\n",
      "loss: 0.483826 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.469789 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.408058 [    0/60000]\n",
      "loss: 0.475864 [ 6400/60000]\n",
      "loss: 0.295325 [12800/60000]\n",
      "loss: 0.544318 [19200/60000]\n",
      "loss: 0.482215 [25600/60000]\n",
      "loss: 0.452903 [32000/60000]\n",
      "loss: 0.410795 [38400/60000]\n",
      "loss: 0.532558 [44800/60000]\n",
      "loss: 0.521690 [51200/60000]\n",
      "loss: 0.456635 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.448537 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.377953 [    0/60000]\n",
      "loss: 0.449806 [ 6400/60000]\n",
      "loss: 0.277015 [12800/60000]\n",
      "loss: 0.513213 [19200/60000]\n",
      "loss: 0.453983 [25600/60000]\n",
      "loss: 0.429853 [32000/60000]\n",
      "loss: 0.382354 [38400/60000]\n",
      "loss: 0.513961 [44800/60000]\n",
      "loss: 0.494735 [51200/60000]\n",
      "loss: 0.437802 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.432581 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.355231 [    0/60000]\n",
      "loss: 0.430557 [ 6400/60000]\n",
      "loss: 0.262642 [12800/60000]\n",
      "loss: 0.487239 [19200/60000]\n",
      "loss: 0.432585 [25600/60000]\n",
      "loss: 0.411058 [32000/60000]\n",
      "loss: 0.360262 [38400/60000]\n",
      "loss: 0.496535 [44800/60000]\n",
      "loss: 0.473550 [51200/60000]\n",
      "loss: 0.421886 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.420099 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.336381 [    0/60000]\n",
      "loss: 0.415424 [ 6400/60000]\n",
      "loss: 0.251835 [12800/60000]\n",
      "loss: 0.463454 [19200/60000]\n",
      "loss: 0.415848 [25600/60000]\n",
      "loss: 0.395381 [32000/60000]\n",
      "loss: 0.342762 [38400/60000]\n",
      "loss: 0.481735 [44800/60000]\n",
      "loss: 0.454920 [51200/60000]\n",
      "loss: 0.408555 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.6%, Avg loss: 0.409870 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.320851 [    0/60000]\n",
      "loss: 0.403954 [ 6400/60000]\n",
      "loss: 0.241872 [12800/60000]\n",
      "loss: 0.443633 [19200/60000]\n",
      "loss: 0.401495 [25600/60000]\n",
      "loss: 0.381099 [32000/60000]\n",
      "loss: 0.329009 [38400/60000]\n",
      "loss: 0.468887 [44800/60000]\n",
      "loss: 0.438335 [51200/60000]\n",
      "loss: 0.396956 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.401356 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.307976 [    0/60000]\n",
      "loss: 0.394425 [ 6400/60000]\n",
      "loss: 0.232555 [12800/60000]\n",
      "loss: 0.425844 [19200/60000]\n",
      "loss: 0.390054 [25600/60000]\n",
      "loss: 0.369140 [32000/60000]\n",
      "loss: 0.317785 [38400/60000]\n",
      "loss: 0.457366 [44800/60000]\n",
      "loss: 0.423603 [51200/60000]\n",
      "loss: 0.386365 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.394088 \n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Optimization Loop\n",
    "def move_to_device(inputs, labels, device):\n",
    "    return inputs.to(device), labels.to(device)\n",
    "\n",
    "def train_loop(dataloader : DataLoader, model : nn.Module, loss_fn : torch.nn.modules.loss , optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train(mode=True)\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X, y = move_to_device(X, y, device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "      \n",
    "def test_loop(dataloader : DataLoader, model : nn.Module, loss_fn : torch.nn.modules.loss):\n",
    "    model.train(mode=False)\n",
    "    model.eval()  # Important for batchnorm, dropout, etc.\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    # No need to compute gradients\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = move_to_device(X, y, device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    \n",
    "epochs = 10\n",
    "for e in range(epochs):\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Done\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
