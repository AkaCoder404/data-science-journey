{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W2/ungraded_labs/C3_W2_Lab_1_imdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGATMXZE4oAD"
   },
   "source": [
    "# Ungraded Lab: Training a binary classifier with the IMDB Reviews Dataset\n",
    "\n",
    "In this lab, you will be building a sentiment classification model to distinguish between positive and negative movie reviews. You will train it on the [IMDB Reviews](http://ai.stanford.edu/~amaas/data/sentiment/) dataset and visualize the word embeddings generated after training. \n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWt8QlOGpy1j"
   },
   "source": [
    "## Download the Dataset\n",
    "\n",
    "First, you will need to fetch the dataset you will be working on. This is hosted via [Tensorflow Datasets](https://www.tensorflow.org/datasets), a collection of prepared datasets for machine learning. If you're running this notebook on your local machine, make sure to have the [`tensorflow-datasets`](https://pypi.org/project/tensorflow-datasets/) package installed before importing it. You can install it via `pip` as shown in the commented cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vqB3GzBorwBh"
   },
   "outputs": [],
   "source": [
    "# Install this package if running on your local machine\n",
    "# !pip install -q tensorflow-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpLsMxO2wDrn"
   },
   "source": [
    "The [`tfds.load`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load) method downloads the dataset into your working directory. You can set the `with_info` parameter to `True` if you want to see the description of the dataset. The `as_supervised` parameter, on the other hand, is set to load the data as `(input, label)` pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_IoM4VFxWpMR"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/georgeli/Documents/Notes/DS/DeeplearnAi_Intro_to_Tensorflow/Unit3/05_C3_W2_Lab_1_imdb.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/georgeli/Documents/Notes/DS/DeeplearnAi_Intro_to_Tensorflow/Unit3/05_C3_W2_Lab_1_imdb.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtfds\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/georgeli/Documents/Notes/DS/DeeplearnAi_Intro_to_Tensorflow/Unit3/05_C3_W2_Lab_1_imdb.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(tensorflow\u001b[39m.\u001b[39m__version__)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/georgeli/Documents/Notes/DS/DeeplearnAi_Intro_to_Tensorflow/Unit3/05_C3_W2_Lab_1_imdb.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Load the IMDB Reviews dataset\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow_datasets/__init__.py:43\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m\"\"\"`tensorflow_datasets` (`tfds`) defines a collection of datasets ready-to-use with TensorFlow.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[39mEach dataset is defined as a `tfds.core.DatasetBuilder`, which encapsulates\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m* [Add a dataset](https://www.tensorflow.org/datasets/add_dataset)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top,g-bad-import-order,wrong-import-position,unused-import\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m# needs to happen before anything else, since the imports below will try to\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# import tensorflow, too.\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m tf_compat\n\u001b[1;32m     44\u001b[0m tf_compat\u001b[39m.\u001b[39mensure_tf_install()\n\u001b[1;32m     46\u001b[0m \u001b[39m# Imports for registration\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow_datasets/core/__init__.py:33\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39metils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mepath\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[1;32m     31\u001b[0m \u001b[39m# pylint: enable=g-bad-import-order\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m community  \u001b[39m# pylint: disable=g-bad-import-order\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_builder\u001b[39;00m \u001b[39mimport\u001b[39;00m BeamBasedBuilder\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_builder\u001b[39;00m \u001b[39mimport\u001b[39;00m BuilderConfig\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow_datasets/core/community/__init__.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# coding=utf-8\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Copyright 2022 The TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m\"\"\"Community dataset API.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommunity\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhuggingface_wrapper\u001b[39;00m \u001b[39mimport\u001b[39;00m mock_builtin_to_use_gfile\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommunity\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhuggingface_wrapper\u001b[39;00m \u001b[39mimport\u001b[39;00m mock_huggingface_import\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommunity\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mload\u001b[39;00m \u001b[39mimport\u001b[39;00m builder_cls_from_module\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow_datasets/core/community/huggingface_wrapper.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39metils\u001b[39;00m \u001b[39mimport\u001b[39;00m epath\n\u001b[1;32m     28\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m dataset_builder\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m dataset_info\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m download\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_builder.py:32\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msix\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m dataset_info\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m decode\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m download\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_info.py:48\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m lazy_imports_lib\n\u001b[1;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m naming\n\u001b[0;32m---> 48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m splits \u001b[39mas\u001b[39;00m splits_lib\n\u001b[1;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[1;32m     50\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeatures\u001b[39;00m \u001b[39mimport\u001b[39;00m feature \u001b[39mas\u001b[39;00m feature_lib\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow_datasets/core/splits.py:32\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39metils\u001b[39;00m \u001b[39mimport\u001b[39;00m epath\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m naming\n\u001b[0;32m---> 32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m proto \u001b[39mas\u001b[39;00m proto_lib\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m units\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow_datasets/core/proto/__init__.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# coding=utf-8\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Copyright 2022 The TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m\"\"\"Public API of the proto package.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproto\u001b[39;00m \u001b[39mimport\u001b[39;00m dataset_info_generated_pb2 \u001b[39mas\u001b[39;00m dataset_info_pb2  \u001b[39m# pylint: disable=line-too-long\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproto\u001b[39;00m \u001b[39mimport\u001b[39;00m feature_generated_pb2 \u001b[39mas\u001b[39;00m feature_pb2  \u001b[39m# pylint: disable=line-too-long\u001b[39;00m\n\u001b[1;32m     21\u001b[0m SplitInfo \u001b[39m=\u001b[39m dataset_info_pb2\u001b[39m.\u001b[39mSplitInfo\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow_datasets/core/proto/dataset_info_generated_pb2.py:30\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[0;32m---> 30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_datasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproto\u001b[39;00m \u001b[39mimport\u001b[39;00m feature_generated_pb2 \u001b[39mas\u001b[39;00m feature__pb2\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_metadata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproto\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv0\u001b[39;00m \u001b[39mimport\u001b[39;00m schema_pb2 \u001b[39mas\u001b[39;00m tensorflow__metadata_dot_proto_dot_v0_dot_schema__pb2\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow_metadata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproto\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv0\u001b[39;00m \u001b[39mimport\u001b[39;00m statistics_pb2 \u001b[39mas\u001b[39;00m tensorflow__metadata_dot_proto_dot_v0_dot_statistics__pb2\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow_datasets/core/proto/feature_generated_pb2.py:45\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m _sym_db \u001b[39m=\u001b[39m _symbol_database\u001b[39m.\u001b[39mDefault()\n\u001b[1;32m     30\u001b[0m DESCRIPTOR \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mFileDescriptor(\n\u001b[1;32m     31\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfeature.proto\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     32\u001b[0m     package\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow_datasets\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     serialized_pb\u001b[39m=\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39mfeature.proto\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x13\u001b[39;00m\u001b[39mtensorflow_datasets\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m\\xa0\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39m\\x46\u001b[39;00m\u001b[39m\\x65\u001b[39;00m\u001b[39m\\x61\u001b[39;00m\u001b[39mturesDict\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x41\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x08\u001b[39;00m\u001b[39m\\x66\u001b[39;00m\u001b[39m\\x65\u001b[39;00m\u001b[39m\\x61\u001b[39;00m\u001b[39mtures\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x03\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m/.tensorflow_datasets.FeaturesDict.FeaturesEntry\u001b[39m\u001b[39m\\x1a\u001b[39;00m\u001b[39mM\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39mFeaturesEntry\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x03\u001b[39;00m\u001b[39mkey\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m+\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39mvalue\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m\\x1c\u001b[39;00m\u001b[39m.tensorflow_datasets.Feature:\u001b[39m\u001b[39m\\x02\u001b[39;00m\u001b[39m\\x38\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m\\xee\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x07\u001b[39;00m\u001b[39m\\x46\u001b[39;00m\u001b[39m\\x65\u001b[39;00m\u001b[39m\\x61\u001b[39;00m\u001b[39mture\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x19\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x11\u001b[39;00m\u001b[39mpython_class_name\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x13\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x64\u001b[39;00m\u001b[39m\\x65\u001b[39;00m\u001b[39mscription\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x0e\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x13\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0b\u001b[39;00m\u001b[39mvalue_range\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x0f\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x38\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39mjson_feature\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m .tensorflow_datasets.JsonFeatureH\u001b[39m\u001b[39m\\x00\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39mfeatures_dict\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x03\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m!.tensorflow_datasets.FeaturesDictH\u001b[39m\u001b[39m\\x00\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x34\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x06\u001b[39;00m\u001b[39mtensor\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x04\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m.tensorflow_datasets.TensorFeatureH\u001b[39m\u001b[39m\\x00\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x36\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x63\u001b[39;00m\u001b[39mlass_label\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m\\x1f\u001b[39;00m\u001b[39m.tensorflow_datasets.ClassLabelH\u001b[39m\u001b[39m\\x00\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39mimage\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x06\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m!.tensorflow_datasets.ImageFeatureH\u001b[39m\u001b[39m\\x00\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39mvideo\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x07\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m!.tensorflow_datasets.VideoFeatureH\u001b[39m\u001b[39m\\x00\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39m\\x61\u001b[39;00m\u001b[39mudio\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x08\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m!.tensorflow_datasets.AudioFeatureH\u001b[39m\u001b[39m\\x00\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m?\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39m\\x62\u001b[39;00m\u001b[39mounding_box\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m\\'\u001b[39;00m\u001b[39m.tensorflow_datasets.BoundingBoxFeatureH\u001b[39m\u001b[39m\\x00\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x30\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x04\u001b[39;00m\u001b[39mtext\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m .tensorflow_datasets.TextFeatureH\u001b[39m\u001b[39m\\x00\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m>\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0b\u001b[39;00m\u001b[39mtranslation\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x0b\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m\\'\u001b[39;00m\u001b[39m.tensorflow_datasets.TranslationFeatureH\u001b[39m\u001b[39m\\x00\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x31\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x08\u001b[39;00m\u001b[39msequence\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m\\x1d\u001b[39;00m\u001b[39m.tensorflow_datasets.SequenceH\u001b[39m\u001b[39m\\x00\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x06\u001b[39;00m\u001b[39mscalar\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m\\x1b\u001b[39;00m\u001b[39m.tensorflow_datasets.ScalarH\u001b[39m\u001b[39m\\x00\u001b[39;00m\u001b[39m\\x42\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x07\u001b[39;00m\u001b[39m\\x63\u001b[39;00m\u001b[39montent\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m\\x1b\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0b\u001b[39;00m\u001b[39mJsonFeature\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x04\u001b[39;00m\u001b[39mjson\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m\\x1b\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39mShape\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mdimensions\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x03\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x03\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m\\x17\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x06\u001b[39;00m\u001b[39mScalar\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39m\\x64\u001b[39;00m\u001b[39mtype\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m[\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39mTensorFeature\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39mshape\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m\\x1a\u001b[39;00m\u001b[39m.tensorflow_datasets.Shape\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39m\\x64\u001b[39;00m\u001b[39mtype\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x10\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x08\u001b[39;00m\u001b[39m\\x65\u001b[39;00m\u001b[39mncoding\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x03\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m!\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mClassLabel\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x13\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0b\u001b[39;00m\u001b[39mnum_classes\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x03\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m\\xa7\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39mImageFeature\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39mshape\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m\\x1a\u001b[39;00m\u001b[39m.tensorflow_datasets.Shape\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39m\\x64\u001b[39;00m\u001b[39mtype\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x17\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0f\u001b[39;00m\u001b[39m\\x65\u001b[39;00m\u001b[39mncoding_format\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x03\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x14\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39muse_colormap\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x04\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x08\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39mlabel\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m\\x1f\u001b[39;00m\u001b[39m.tensorflow_datasets.ClassLabel\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m\\x92\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39mVideoFeature\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39mshape\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m\\x1a\u001b[39;00m\u001b[39m.tensorflow_datasets.Shape\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39m\\x64\u001b[39;00m\u001b[39mtype\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x17\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0f\u001b[39;00m\u001b[39m\\x65\u001b[39;00m\u001b[39mncoding_format\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x03\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x14\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39muse_colormap\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x04\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x08\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x19\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x11\u001b[39;00m\u001b[39m\\x66\u001b[39;00m\u001b[39m\\x66\u001b[39;00m\u001b[39mmpeg_extra_args\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x03\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m\\x84\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0c\u001b[39;00m\u001b[39m\\x41\u001b[39;00m\u001b[39mudioFeature\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39mshape\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m\\x1a\u001b[39;00m\u001b[39m.tensorflow_datasets.Shape\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39m\\x64\u001b[39;00m\u001b[39mtype\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x13\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x66\u001b[39;00m\u001b[39mile_format\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x03\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x13\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0b\u001b[39;00m\u001b[39msample_rate\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x04\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x03\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x10\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x08\u001b[39;00m\u001b[39m\\x65\u001b[39;00m\u001b[39mncoding\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39mN\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x42\u001b[39;00m\u001b[39moundingBoxFeature\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39mshape\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m\\x1a\u001b[39;00m\u001b[39m.tensorflow_datasets.Shape\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x05\u001b[39;00m\u001b[39m\\x64\u001b[39;00m\u001b[39mtype\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x0b\u001b[39;00m\u001b[39mTextFeature\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mO\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39mTranslationFeature\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x11\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mlanguages\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x03\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\x12\u001b[39;00m\u001b[39m&\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x1e\u001b[39;00m\u001b[39mvariable_languages_per_example\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x08\u001b[39;00m\u001b[39m\\\"\u001b[39;00m\u001b[39mI\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x08\u001b[39;00m\u001b[39mSequence\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x07\u001b[39;00m\u001b[39m\\x66\u001b[39;00m\u001b[39m\\x65\u001b[39;00m\u001b[39m\\x61\u001b[39;00m\u001b[39mture\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x0b\u001b[39;00m\u001b[39m\\x32\u001b[39;00m\u001b[39m\\x1c\u001b[39;00m\u001b[39m.tensorflow_datasets.Feature\u001b[39m\u001b[39m\\x12\u001b[39;00m\u001b[39m\\x0e\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\x06\u001b[39;00m\u001b[39mlength\u001b[39m\u001b[39m\\x18\u001b[39;00m\u001b[39m\\x02\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\x01\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m\\x03\u001b[39;00m\u001b[39m\\x42\u001b[39;00m\u001b[39m\\x03\u001b[39;00m\u001b[39m\\xf8\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m\\x01\u001b[39;00m\u001b[39m\\x62\u001b[39;00m\u001b[39m\\x06\u001b[39;00m\u001b[39mproto3\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     38\u001b[0m _FEATURESDICT_FEATURESENTRY \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mDescriptor(\n\u001b[1;32m     39\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFeaturesEntry\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     40\u001b[0m     full_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow_datasets.FeaturesDict.FeaturesEntry\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     41\u001b[0m     filename\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     42\u001b[0m     file\u001b[39m=\u001b[39mDESCRIPTOR,\n\u001b[1;32m     43\u001b[0m     containing_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     44\u001b[0m     fields\u001b[39m=\u001b[39m[\n\u001b[0;32m---> 45\u001b[0m         _descriptor\u001b[39m.\u001b[39;49mFieldDescriptor(\n\u001b[1;32m     46\u001b[0m             name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mkey\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     47\u001b[0m             full_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtensorflow_datasets.FeaturesDict.FeaturesEntry.key\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     48\u001b[0m             index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     49\u001b[0m             number\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     50\u001b[0m             \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m9\u001b[39;49m,\n\u001b[1;32m     51\u001b[0m             cpp_type\u001b[39m=\u001b[39;49m\u001b[39m9\u001b[39;49m,\n\u001b[1;32m     52\u001b[0m             label\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     53\u001b[0m             has_default_value\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     54\u001b[0m             default_value\u001b[39m=\u001b[39;49m\u001b[39mb\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mdecode(\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m     55\u001b[0m             message_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     56\u001b[0m             enum_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     57\u001b[0m             containing_type\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     58\u001b[0m             is_extension\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     59\u001b[0m             extension_scope\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     60\u001b[0m             serialized_options\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     61\u001b[0m             file\u001b[39m=\u001b[39;49mDESCRIPTOR),\n\u001b[1;32m     62\u001b[0m         _descriptor\u001b[39m.\u001b[39mFieldDescriptor(\n\u001b[1;32m     63\u001b[0m             name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     64\u001b[0m             full_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow_datasets.FeaturesDict.FeaturesEntry.value\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     65\u001b[0m             index\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     66\u001b[0m             number\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     67\u001b[0m             \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m11\u001b[39m,\n\u001b[1;32m     68\u001b[0m             cpp_type\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m     69\u001b[0m             label\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     70\u001b[0m             has_default_value\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m             default_value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m             message_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     73\u001b[0m             enum_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     74\u001b[0m             containing_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m             is_extension\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     76\u001b[0m             extension_scope\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     77\u001b[0m             serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     78\u001b[0m             file\u001b[39m=\u001b[39mDESCRIPTOR),\n\u001b[1;32m     79\u001b[0m     ],\n\u001b[1;32m     80\u001b[0m     extensions\u001b[39m=\u001b[39m[],\n\u001b[1;32m     81\u001b[0m     nested_types\u001b[39m=\u001b[39m[],\n\u001b[1;32m     82\u001b[0m     enum_types\u001b[39m=\u001b[39m[],\n\u001b[1;32m     83\u001b[0m     serialized_options\u001b[39m=\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m8\u001b[39m\u001b[39m\\001\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m     84\u001b[0m     is_extendable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     85\u001b[0m     syntax\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mproto3\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     86\u001b[0m     extension_ranges\u001b[39m=\u001b[39m[],\n\u001b[1;32m     87\u001b[0m     oneofs\u001b[39m=\u001b[39m[],\n\u001b[1;32m     88\u001b[0m     serialized_start\u001b[39m=\u001b[39m\u001b[39m122\u001b[39m,\n\u001b[1;32m     89\u001b[0m     serialized_end\u001b[39m=\u001b[39m\u001b[39m199\u001b[39m,\n\u001b[1;32m     90\u001b[0m )\n\u001b[1;32m     92\u001b[0m _FEATURESDICT \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mDescriptor(\n\u001b[1;32m     93\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFeaturesDict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     94\u001b[0m     full_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow_datasets.FeaturesDict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m     serialized_end\u001b[39m=\u001b[39m\u001b[39m199\u001b[39m,\n\u001b[1;32m    129\u001b[0m )\n\u001b[1;32m    131\u001b[0m _FEATURE \u001b[39m=\u001b[39m _descriptor\u001b[39m.\u001b[39mDescriptor(\n\u001b[1;32m    132\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFeature\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    133\u001b[0m     full_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensorflow_datasets.Feature\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m     serialized_end\u001b[39m=\u001b[39m\u001b[39m952\u001b[39m,\n\u001b[1;32m    411\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/google/protobuf/descriptor.py:561\u001b[0m, in \u001b[0;36mFieldDescriptor.__new__\u001b[0;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m, name, full_name, index, number, \u001b[39mtype\u001b[39m, cpp_type, label,\n\u001b[1;32m    556\u001b[0m             default_value, message_type, enum_type, containing_type,\n\u001b[1;32m    557\u001b[0m             is_extension, extension_scope, options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    558\u001b[0m             serialized_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    559\u001b[0m             has_default_value\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, containing_oneof\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json_name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    560\u001b[0m             file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, create_key\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):  \u001b[39m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m   _message\u001b[39m.\u001b[39;49mMessage\u001b[39m.\u001b[39;49m_CheckCalledFromGeneratedFile()\n\u001b[1;32m    562\u001b[0m   \u001b[39mif\u001b[39;00m is_extension:\n\u001b[1;32m    563\u001b[0m     \u001b[39mreturn\u001b[39;00m _message\u001b[39m.\u001b[39mdefault_pool\u001b[39m.\u001b[39mFindExtensionByName(full_name)\n",
      "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the IMDB Reviews dataset\n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "J3PEarpKw9_j"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/georgeli/Documents/Notes/DS/DeeplearnAi_Intro_to_Tensorflow/Unit3/05_C3_W2_Lab_1_imdb.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/georgeli/Documents/Notes/DS/DeeplearnAi_Intro_to_Tensorflow/Unit3/05_C3_W2_Lab_1_imdb.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Print information about the dataset\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/georgeli/Documents/Notes/DS/DeeplearnAi_Intro_to_Tensorflow/Unit3/05_C3_W2_Lab_1_imdb.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(info)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'info' is not defined"
     ]
    }
   ],
   "source": [
    "# Print information about the dataset\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLRAoHil5poj"
   },
   "source": [
    "As you can see in the output above, there is a total of 100,000 examples in the dataset and it is split into `train`, `test` and `unsupervised` sets. For this lab, you will only use `train` and `test` sets because you will need labeled examples to train your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EzNDkdkpvrv"
   },
   "source": [
    "## Split the dataset\n",
    "\n",
    "If you try printing the `imdb` dataset that you downloaded earlier, you will see that it contains the dictionary that points to [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) objects. You will explore more of this class and its API in Course 4 of this specialization. For now, you can just think of it as a collection of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tA5397cs-EwN"
   },
   "outputs": [],
   "source": [
    "# Print the contents of the dataset you downloaded\n",
    "print(imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4oiQ0waBduJ"
   },
   "source": [
    "You can preview the raw format of a few examples by using the [`take()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take) method and iterating over it as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NgUwTDu7Q1O"
   },
   "outputs": [],
   "source": [
    "# Take 2 training examples and print its contents\n",
    "for example in imdb['train'].take(2):\n",
    "  print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOtXX2gxB8pe"
   },
   "source": [
    "You can see that each example is a 2-element tuple of tensors containing the text first, then the label (shown in the `numpy()` property). The next cell below will take all the `train` and `test` sentences and labels into separate lists so you can preprocess the text and feed it to the model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHQ2Ko0zl7M4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get the train and test sets\n",
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "# Initialize sentences and labels lists\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "# Loop over all training examples and save the sentences and labels\n",
    "for s,l in train_data:\n",
    "  training_sentences.append(s.numpy().decode('utf8'))\n",
    "  training_labels.append(l.numpy())\n",
    "\n",
    "# Loop over all test examples and save the sentences and labels\n",
    "for s,l in test_data:\n",
    "  testing_sentences.append(s.numpy().decode('utf8'))\n",
    "  testing_labels.append(l.numpy())\n",
    "\n",
    "# Convert labels lists to numpy array\n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePTIgXj3q8Sg"
   },
   "source": [
    "## Generate Padded Sequences\n",
    "\n",
    "Now you can do the text preprocessing steps you've learned last week. You will tokenize the sentences and pad them to a uniform length. We've separated the parameters into its own code cell below so it will be easy for you to tweak it later if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lggoZqYUGYgX"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "vocab_size = 10000\n",
    "max_length = 120\n",
    "embedding_dim = 16\n",
    "trunc_type='post'\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7n15yyMdmoH1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Initialize the Tokenizer class\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "\n",
    "# Generate the word index dictionary for the training sentences\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Generate and pad the training sequences\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "# Generate and pad the test sequences\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length, truncating=trunc_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2rCmp7ArGL_"
   },
   "source": [
    "## Build and Compile the Model\n",
    "\n",
    "With the data already preprocessed, you can proceed to building your sentiment classification model. The input will be an [`Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer. The main idea here is to represent each word in your vocabulary with vectors. These vectors have trainable weights so as your neural network learns, words that are most likely to appear in a positive tweet will converge towards similar weights. Similarly, words in negative tweets will be clustered more closely together. You can read more about word embeddings [here](https://www.tensorflow.org/text/guide/word_embeddings).\n",
    "\n",
    "After the `Embedding` layer, you will flatten its output and feed it into a `Dense` layer. You will explore other architectures for these hidden layers in the next labs.\n",
    "\n",
    "The output layer would be a single neuron with a sigmoid activation to distinguish between the 2 classes. As is typical with binary classifiers, you will use the `binary_crossentropy` as your loss function while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NEpdhb8AxID"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Setup the training parameters\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8gbnoRdqp8O"
   },
   "source": [
    "## Train the Model\n",
    "\n",
    "Next, of course, is to train your model. With the current settings, you will get near perfect training accuracy after just 5 epochs but the validation accuracy will plateau at around 83%. See if you can still improve this by adjusting some of the parameters earlier (e.g. the `vocab_size`, number of `Dense` neurons, number of epochs, etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5LLrXC-uNX6"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mroDvjEJqwm4"
   },
   "source": [
    "## Visualize Word Embeddings\n",
    "\n",
    "After training, you can visualize the trained weights in the `Embedding` layer to see words that are clustered together. The [Tensorflow Embedding Projector](https://projector.tensorflow.org/) is able to reduce the 16-dimension vectors you defined earlier into fewer components so it can be plotted in the projector. First, you will need to get these weights and you can do that with the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAmjJqEyCOF_"
   },
   "outputs": [],
   "source": [
    "# Get the embedding layer from the model (i.e. first layer)\n",
    "embedding_layer = model.layers[0]\n",
    "\n",
    "# Get the weights of the embedding layer\n",
    "embedding_weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "# Print the shape. Expected is (vocab_size, embedding_dim)\n",
    "print(embedding_weights.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEuG9AqIuF6i"
   },
   "source": [
    "You will need to generate two files:\n",
    "\n",
    "* `vecs.tsv` - contains the vector weights of each word in the vocabulary\n",
    "* `meta.tsv` - contains the words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1u4Ty097uRYP"
   },
   "source": [
    "For this, it is useful to have `reverse_word_index` dictionary so you can quickly lookup a word based on a given index. For example, `reverse_word_index[1]` will return your OOV token because it is always at index = 1. Fortunately, the `Tokenizer` class already provides this dictionary through its `index_word` property. Yes, as the name implies, it is the reverse of the `word_index` property which you used earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pPhhHqvxvS8f"
   },
   "outputs": [],
   "source": [
    "# Get the index-word dictionary\n",
    "reverse_word_index = tokenizer.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykM0Q9ThvszB"
   },
   "source": [
    "Now you can start the loop to generate the files. You will loop `vocab_size-1` times, skipping the `0` key because it is just for the padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmB0Uxk0ycP6"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Open writeable files\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "# Initialize the loop. Start counting at `1` because `0` is just for the padding\n",
    "for word_num in range(1, vocab_size):\n",
    "\n",
    "  # Get the word associated at the current index\n",
    "  word_name = reverse_word_index[word_num]\n",
    "\n",
    "  # Get the embedding weights associated with the current index\n",
    "  word_embedding = embedding_weights[word_num]\n",
    "\n",
    "  # Write the word name\n",
    "  out_m.write(word_name + \"\\n\")\n",
    "\n",
    "  # Write the word embedding\n",
    "  out_v.write('\\t'.join([str(x) for x in word_embedding]) + \"\\n\")\n",
    "\n",
    "# Close the files\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3t92Osu3u8Qh"
   },
   "source": [
    "When running this on Colab, you can run the code below to download the files. Otherwise, you can see the files in your current working directory and download it manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDeqpOCVydtq"
   },
   "outputs": [],
   "source": [
    "# Import files utilities in Colab\n",
    "try:\n",
    "  from google.colab import files\n",
    "except ImportError:\n",
    "  pass\n",
    "\n",
    "# Download the files\n",
    "else:\n",
    "  files.download('vecs.tsv')\n",
    "  files.download('meta.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRV8ag3nyAOb"
   },
   "source": [
    "Now you can go to the [Tensorflow Embedding Projector](https://projector.tensorflow.org/) and load the two files you downloaded to see the visualization. You can search for words like `worst` and `fantastic` and see the other words closely located to these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GOiu0WHzMzk"
   },
   "source": [
    "## Wrap Up\n",
    "\n",
    "In this lab, you were able build a simple sentiment classification model and train it on preprocessed text data. In the next lessons, you will revisit the Sarcasm Dataset you used in Week 1 and build a model to train on it."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C3_W2_Lab_1_imdb.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
